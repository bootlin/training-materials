\subchapter
{System wide profiling and tracing}
{Objectives:
  \begin{itemize}
    \item System profiling with {\em trace-cmd} and {\em kernelshark}.
    \item Tracing and visualizing system activity using {\em LTTng} and
          {\em trace-compass}.
    \item (Bonus) System profiling with {\em perf} and FlameGraphs.
  \end{itemize}
}

\section{ftrace \& uprobes}

First of all, we will start a small program on the target using the following command:

\begin{bashinput}
$ mystery_program 1000 200 2 &
\end{bashinput}

In order to trace a full system, we can use ftrace. However, if we want to trace
the userspace, we'll need to add new tracepoints using uprobes. This can be done
manually with the uprobe sysfs interface or using \code{perf probe}.

Before starting to profile, we will compile our program to be instrumented.
On your development host, run:

\begin{bashinput}
$ cd /home/$USER/debugging-labs/nfsroot/root/system_profiling
$ make
\end{bashinput}

On the target, we will create a uprobe in the main function of the
\code{crc_random} program each time a crc is computed. First, let's list the line
numbers that are recognized by perf to add a uprobe:

\begin{bashinput}
$ cd /root/system_profiling
$ perf probe --source=./ -x ./crc_random -L main
\end{bashinput}

{\em Notes:
 \begin{itemize}
  \item In order to be able to add such userspace probe, perf needs to access
symbols and source file
  \item If perf output is filled with misinterpreted ANSI escape sequences, you
  can append \code{| cat} to the command
 \end{itemize}}

Then, we can create a uprobe and capture the crc value using:

\begin{bashinput}
$ perf probe --source=./ -x ./crc_random main:35 crc
\end{bashinput}

We can finally trace the application using \code{trace-cmd} with this event. We will
use the remote tracing capabilities of \code{trace-cmd} to do that. First, we will
start the \code{trace-cmd} server on the desktop platform:

\begin{bashinput}
trace-cmd listen -p 4567
\end{bashinput}

Then, on the target, run \code{trace-cmd} with the \code{-N} parameter to
specify the remote trace-cmd server. Interrupt it after about 50 CRC
computations in order to have a meaningful trace.

\begin{bashinput}
$ trace-cmd record -N 192.168.0.1:4567 -e probe_crc_random:main_L35 ./crc_random
^C
\end{bashinput}

Then, using kernelshark tool on the host, analyze the trace:

\begin{bashinput}
$ sudo apt install kernelshark
$ kernelshark
\end{bashinput}

We can see that something is wrong, our process does not seem to compute crc at
a fixed rate. Let's trace the \code{sched_switch} events to see what is happening:

\begin{bashinput}
$ trace-cmd record -N 192.168.0.1:4567 -e sched_switch -e probe_crc_random:main_L35 ./crc_random
^C
\end{bashinput}

Reanalyze the traces with kernelshark and try to understand what is going on.

\section{LTTng}

In order to observe our program performance, we want to instrument it with
tracepoints. We would like to know how much time it takes to compute the
crc32 of a specific buffer.

In order to do so, add tracepoints to your program which will allow to measure
this. We'll add 2 tracepoints:

\begin{itemize}
  \item One for the start of crc32 computation (\code{compute_crc_start})
    without any arguments.
  \item Another for the end of crc32 computation (\code{compute_crc_end}) with
      a crc \code{int} argument that will be displayed as an hexadecimal integer
      using \code{lttng_ust_field_integer_hex()} output formatter.
\end{itemize}

In order to create these tracepoints easily, we will create a
\code{crc_random-tp.tp} file and generate the tracepoints using
\code{lttng-gen-tp}. In order to install this tool, the \code{liblttng-ust-dev}
should be installed in your development host:

\begin{bashinput}
sudo apt install liblttng-ust-dev
\end{bashinput}

These tracepoints should belong to the \code{crc_random}
provider namespace. Once written, you can modify the \code{Makefile} by adding a
new rule and modifying the existing one to generate the tracepoints files using
\code{lttng-gen-tp}:

\begin{bashinput}
  CC=${CROSS_COMPILE}gcc

  crc_random: crc_random.c crc_random-tp.o
      ${CC} $^ -g3 -llttng-ust -o $@

  crc_random-tp.o: crc_random-tp.tp
      CC=${CC} lttng-gen-tp $<
\end{bashinput}

You can then use the new tracepoints in your program to trace specific points
of execution as requested.

Finally, on the target, enable the program tracepoints, run it and collect
tracepoints. We are going to do that remotely using the \code{lttng-relayd} tool
on the remote computer:

\begin{bashinput}
$ sudo apt install lttng-tools
$ lttng-relayd --output=$PWD/traces
\end{bashinput}

Then on the target, start the trace acquisition using:

\begin{bashinput}
$ lttng-sessiond --daemonize
$ lttng create crc_session --set-url=net://192.168.0.1
$ lttng enable-event --userspace crc_random:compute_crc_start
$ lttng enable-event --userspace crc_random:compute_crc_end
$ lttng enable-event --kernel sched_switch
$ lttng start
$ ./crc_random
$ # Interrupt the program after a while
$ lttng destroy
\end{bashinput}

{\em From there, you can safely stop \code{lttng-relayd} on your computer}.

Once finished, the traces will be visible in \code{$PWD/traces/<hostname>/<session>}
on the remote computer.
In our case, the hostname is buildroot so traces will be
located in \code{$PWD/traces/buildroot/<session>}

Using \code{babeltrace2}, you can display the raw traces that were acquired directly
on your development host:
\begin{bashinput}
$ sudo apt install babeltrace2
$ babeltrace2 $PWD/traces/buildroot/<session>/
\end{bashinput}

In order to analyze our traces more visually, we are going to use tracecompass.
Download \code{tracecompass} latest version and extract it on your host using:

\begin{bashinput}
$ wget https://ftp.fau.de/eclipse/tracecompass/releases/8.1.0/rcp/trace-compass-8.1.0-20220919-0815-linux.gtk.x86_64.tar.gz
$ tar -xvf trace-compass-*.tar.gz
\end{bashinput}

Run it
\begin{bashinput}
$ cd trace-compass*
$ ./tracecompass
\end{bashinput}

To load the collected data in tracecompass, use the \code{File ->
  Import...} menu command, and in the \code{Trace Import} form:
\begin{itemize}
  \item enable the \code{Select root directory} option (if not yet enabled)
    and use the \code{Browse} button to and open the
    \code{traces/buildroot/crc_session-*} directory
  \item enable the \code{Create experiment} checkbox and name the
    experiment \code{debugging_lab}
  \item press \code{Finish} to load the data and get back to the main
    window
\end{itemize}

This loads both the kernel and the user space traces and create a new
``experiment'' that merges them both. In the left pane, open the
\code{Tracing -> Experiments [1]} hierarchy and double click on the
\code{debugging_lab [2]} item to display the merged trace. Explore the
interface, and try to follow the task execution on both the Resources view
and in the Control flow one.

\section{(Bonus) System profiling with {\em perf} and FlameGraphs}

In order to profile the whole system, we are going to use perf and try to find
the function that takes most of the time executing.

First of all, we will run a global recording of functions and their backtrace on
(all CPUs) during 10 seconds using the following command on the target:

\begin{bashinput}
$ perf record -F 99 -g -- sleep 10
\end{bashinput}

This command will generate a \code{perf.data} file that can be used on a remote
computer for further analysis. Copy that file and fix the permissions using
\code{chown}:

\begin{bashinput}
$ cd /home/$USER/debugging-labs/buildroot/output/build/linux-5.13/
$ sudo cp /home/$USER/debugging-labs/nfsroot/root/system_profiling/perf.data .
$ sudo chown $USER:$USER perf.data
\end{bashinput}

We will then use perf report to visualize the aquired data:

\begin{bashinput}
$ perf report --symfs=/home/$USER/debugging-labs/buildroot/output/staging/
  -k /home/$USER/debugging-labs/buildroot/output/build/linux-5.13/vmlinux
\end{bashinput}

Another useful tool for performance analysis is flamegraphs. Latest perf
version includes a builtin support for flamegraphs but the template is not
available on debian so we will use another support provided by Brendan Gregg
scripts.

\begin{bashinput}
$ git clone https://github.com/brendangregg/FlameGraph.git
$ perf script | ./FlameGraph/stackcollapse-perf.pl | ./FlameGraph/flamegraph.pl > flame.html
\end{bashinput}

Using this flamegraph, analyze the system load.

{\em TIP: if the generated graph is not relevant/missing symbols, you may try to to all symbols
translation in the target before generating the graph on host, i.e use perf script on the target,
and bring back the result on host and provide it to Flamegraph scripts}

You can also generate a Flamegraph on previous labs: for example you can re-do a recording on
\code{png_convert} from the application profiling lab, generate the corresponding Flamegraph and confirm
your observations about most time-consuming functions.

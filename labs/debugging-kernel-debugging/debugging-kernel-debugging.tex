\subchapter
{Kernel debugging}
{Objectives:
  \begin{itemize}
    \item Debugging locks and sleeps mistakes using {\em PROVE\_LOCKING} and {\em
    DEBUG\_ATOMIC\_SLEEP} options.
    \item Find a module memory leak using {\em kmemleak}.
    \item Analyzing an {\em oops}.
    \item Debugging with {\em KGDB}.
    \item Setting up {\em Kexec \& kdump}.
  \end{itemize}
}

\section{Locking and sleeps problems}

\kconfig{CONFIG_PROVE_LOCKING} and \kconfig{CONFIG_DEBUG_ATOMIC_SLEEP} have been
enabled in the provided kernel image.
First, compile the module on your development host using the following command line:

\begin{bashinput}
$ cd /home/$USER/debugging-labs/nfsroot/root/locking
$ export CROSS_COMPILE=/home/$USER/debugging-labs/buildroot/output/host/bin/arm-linux-
$ export ARCH=arm
$ export KDIR=/home/$USER/debugging-labs/buildroot/output/build/linux-5.13/
$ make
\end{bashinput}

On the target, load the \code{locking.ko} module and look at the output in dmesg:

\begin{bashinput}
# cd /root/locking
# insmod locking_test.ko
# dmesg
\end{bashinput}

Once analyzed, unload the module. Try to understand and fix all the problems that
have been reported by the \code{lockdep} system.

\section{Kmemleak}

The provided kernel image contains kmemleak but it is disabled by default to
avoid having a large overhead. In order to enable it, reboot the target and enable
kmemleak by adding \code{kmemleak=on} on the command line. Interrupt U-Boot at
reboot and modify the \code{bootargs} variable:

\begin{bashinput}
STM32MP> env edit bootargs
STM32MP> <existing bootargs> kmemleak=on
STM32MP> boot
\end{bashinput}

Then compile the dummy test module on your development host:

\begin{bashinput}
$ cd /home/$USER/debugging-labs/nfsroot/root/kmemleak
$ export CROSS_COMPILE=/home/$USER/debugging-labs/buildroot/output/host/bin/arm-linux-
$ export ARCH=arm
$ export KDIR=/home/$USER/debugging-labs/buildroot/output/build/linux-5.13/
$ make
\end{bashinput}

On the target, load the \code{kmemleak_test.ko} and trigger an immediate
kmemleak scan using:

\begin{bashinput}
# cd /root/kmemleak
# insmod kmemleak_test.ko
# rmmod kmemleak_test
# echo scan > /sys/kernel/debug/kmemleak
\end{bashinput}

Note that you might need to run the \code{scan} command several times
before it detects a leakage due to memory still containing references to
the leaked pointer. Soon after that, the kernel will report that some leaks
have been identified. Display them and analyze them using:

\begin{bashinput}
# cat /sys/kernel/debug/kmemleak
\end{bashinput}

You will see that the symbols addresses do not make sense. This is due to the
\code{kptr_restrict} configuration which must be change to allow displaying
pointer addresses. To do so, use the following command on the target:

\begin{bashinput}
# sysctl kernel.kptr_restrict=1
\end{bashinput}

You can use \code{addr2line} to identify the location in source code of the
lines that did cause the reports. You may need to substract module loading address:
you can guess the address by taking a look at \code{/proc/modules} while the module
is loaded.

You will also notice other memory leaks that are actually some real memory leaks
that did exist in the 5.13 kernel version!

\section{OOPS analysis}
We noticed that the watchdog command generated a crash on the kernel. In order
to reproduce the crash, run the following command on the target:

\begin{bashinput}
$ watchdog -T 10 -t 5 /dev/watchdog0
\end{bashinput}

Immediatly after executing this commands, you'll see that the kernel will report
an OOPS!

\subsection{Analyzing the crash message}

Analyze the crash message carefully. Knowing that on ARM, the \code{PC}
register contains the location of the instruction being executed, find
in which function does the crash happen, and what the function call
stack is.

Using Elixir (\url{https://elixir.bootlin.com/linux/latest/source}) or the
kernel source code, have a look at the definition of this function. In most
cases, a careful review of the driver source code is enough to understand the
issue. But not in that case!

\subsection{Locating the exact line where the error happens}

Even if you already found out which instruction caused the crash, it's
useful to use information in the crash report.

If you look again, the report tells you at what offset in the function
this happens. We will disassemble the code for this function to understand
exactly where the issue happened.

That is where we need a kernel compiled with \kconfig{CONFIG_DEBUG_INFO},
which is already enabled in the kernel we compiled in the initial lab.
This way, the kernel vmlinux file is compiled with \code{-g} compiler flag,
which adds a lot of debugging information (matching between source code
lines and assembly for instance).

Using \code{addr2line}, find the exact source code line were the crash happened.
For that, you can use the following command on your development host:

\begin{bashinput}
$ addr2line -e /home/$USER/debugging-labs/buildroot/output/build/linux-5.13/vmlinux
  -a <crash_address>
\end{bashinput}

We can even go a step further and use the cross GDB to open vmlinux and
locate the function and corresponding offset in assembly

\begin{bashinput}
$ ${CROSS_COMPILE}gdb /home/$USER/debugging-labs/buildroot/output/build/linux-5.13/vmlinux
(gdb) disassemble <function>
\end{bashinput}

This can also be done automatically using \code{decode_stacktrace.sh}. First,
copy/paste the OOPS message into the \code{~/debugging-labs/oops.txt} file.
Then, using the script provided by the kernel, execute the following command:

\begin{bashinput}
$ cd /home/$USER/debugging-labs/buildroot/output/build/linux-5.13/
$ export ARCH=arm
$ export CROSS_COMPILE=/home/$USER/debugging-labs/buildroot/output/host/bin/arm-linux-
$ ./scripts/decode_stacktrace.sh vmlinux < ~/debugging-labs/oops.txt
\end{bashinput}

\section{KGDB debugging}
In order to debug this OOPS, we'll use KGDB which is an in-kernel debugger.
The provided image already contains the necessary KGDB support and the watchdog
has been disabled to avoid rebooting while debugging. In order to use KGDB and
the console simultaneously, compile and run kdmx on your development host:

\begin{bashinput}
$ cd /home/$USER/debugging-labs
$ git clone https://git.kernel.org/pub/scm/utils/kernel/kgdb/agent-proxy.git
$ cd agent-proxy/kdmx
$ make
$ ./kdmx -n -d -p/dev/ttyACM0 -b115200
serial port: /dev/ttyACM0
Initalizing the serial port to 115200 8n1
/dev/pts/7 is slave pty for terminal emulator
/dev/pts/8 is slave pty for gdb

Use <ctrl>C to terminate program
\end{bashinput}

Note: the slave ports number will depend on the run.

\textbf{Important: before using \code{/dev/pts/7} and \code{/dev/pts/8}, the
picocom process that did open \code{/dev/ttyACM0} must be closed!}

On the target, setup KGDB by setting the console to be used for that purpose in
kgdboc module parameters:

\begin{bashinput}
$ echo ttySTM0 > /sys/module/kgdboc/parameters/kgdboc
\end{bashinput}

Once done, trigger the crash by running the watchdog command, the system will
automatically wait for a debugger to be attached. Run the cross GDB and
attach a gdb process to KGDB with the following command:

\begin{bashinput}
$ ${CROSS_COMPILE}gdb ${HOME}/debugging-labs/buildroot/output/build/linux-5.13/vmlinux
(gdb) target remote /dev/pts/8
\end{bashinput}

{\em TIP: in order to allow auto-loading of python scripts, you can add
\code{set auto-load safe-path /} in your .gdbinit file}

First of all, confirm with GDB the information that were previously obtained
post-crash. This will allow you to also display variables values. Starting from that
point, we will add a breakpoint on the \code{watchdog_set_drvdata()} function.
However, this function is called early in boot so we will need to actually
attach with KGDB at boot time. To do so, we'll modify the bootargs to specify
that. In U-Boot, add the following arguments to bootargs using \code{env edit}:

\begin{bashinput}
STM32MP> env edit bootargs
STM32MP> <existing bootargs> kgdboc=ttySTM0,115200 kgdbwait
STM32MP> boot
\end{bashinput}

Then the kernel will halt during boot waiting for a GDB process to be attached.
Attach gdb client from your development host using the same command that was previously
used:

\begin{bashinput}
$ ${CROSS_COMPILE}gdb /home/$USER/debugging-labs/buildroot/output/build/linux-5.13/vmlinux
(gdb) target remote /dev/pts/8
\end{bashinput}

Note: if you prefer using gdb-multiarch instead of the cross-gdb we have
built and do not specify a file to be used, gdb-multiarch won't be able to
detect the architecture automatically and the target command will fail. In
that case, you can set the architecture using:

\begin{bashinput}
(gdb) set arch arm
(gdb) set gnutarget elf32-littlearm
\end{bashinput}

Before continuing the execution, add a breakpoint on
\code{watchdog_set_drvdata()} using the \code{break} GDB command and then
continue the execution using the \code{continue} command

\begin{bashinput}
(gdb) break watchdog_set_drvdata
(gdb) continue
\end{bashinput}

Analyze the subsequent calls and find the place where the driver data are
clobbered.

TIP: you can fix the problem in "live" by modifying the content of the
\code{wdd->driver_data} variable directly using the following command:

\begin{bashinput}
(gdb) p/x var=hex_value
\end{bashinput}

Use it to set the variable with the previous value that was used before getting
clobbered with NULL. Once done, continue the execution and verify that you fixed
the problem using the \code{watchdog} command.

{\em Note: In theory, we could have added a watchpoint to watch the address that
was modified but the ARM32 platform does not provide watchpoint support with
KGDB.}

\subsection{Debugging a module}

KGDB also allows to debug modules and thanks to the GDB python scripts
(\code{lx-symbols}) mainly, it is as easy as debugging kernel core code. In
order to test that feature, we are going to compile a test module and break on
it. On your development host, build the module:

\begin{bashinput}
$ cd /home/$USER/debugging-labs/nfsroot/root/kgdb
$ export CROSS_COMPILE=/home/$USER/debugging-labs/buildroot/output/host/bin/arm-linux-
$ export ARCH=arm
$ export KDIR=/home/$USER/debugging-labs/buildroot/output/build/linux-5.13/
$ make
\end{bashinput}

Then on the target, insert the module using insmod:
\begin{bashinput}
# cd /root/kgdb
# insmod kgdb_test.ko
\end{bashinput}

We can now enter KGDB mode and attach the external gdb to it. We will do
that using the magic SySrq 'g' key. Before that, ensure the tty
device for gdb already set, or set it now:

\begin{bashinput}
# echo ttySTM0 > /sys/module/kgdboc/parameters/kgdboc
# echo g > /proc/sysrq-trigger
\end{bashinput}

The kernel will then enter KGDB mode and will wait for a gdb connection. On the
development platform, start it and attach to the target:

\begin{bashinput}$
$ ${CROSS_COMPILE}gdb /home/$USER/debugging-labs/buildroot/output/build/linux-5.13/vmlinux
(gdb) target remote /dev/pts/8
\end{bashinput}

Then you will need to execute the \code{lx-symbols} command in gdb to reload the
symbols from the module. You'll also need to pass a list of path that contains
the external modules:

\begin{bashinput}
(gdb) lx-symbols /home/${USER}/debugging-labs/nfsroot/root/kgdb_test/
loading vmlinux
scanning for modules in /home/<user>/debugging-labs/nfsroot/root
loading @0xbf000000: /home/<user>/debugging-labs/nfsroot/root/kgdb_test/kgdb_test.ko
\end{bashinput}

{\em NOTE: If KGDB was already connected and the lx scripts were loaded, then
\code{lx-symbols} will be run automatically on module loading.}

Finally, add a breakpoint right after the \code{pr_debug()} call and continue
the execution to trigger it:

\begin{bashinput}
(gdb) break kgdb_test.c:17
(gdb) continue
\end{bashinput}

At some point, the breakpoint will be triggered. Try to display the variable
\code{i} to display the current loop value.

Note: Due to a GDB bug, sometimes, gdb will crash when continuing. You can
use a temporary breakpoint using the gdb \code{tbreak} command to workaround
this problem.

Bonus: as a side quest you can try to enable the \code{pr_debug()} call using
the dynamic debug feature of the kernel. This can be done using the
\code{/proc/dynamic_debug/control} file.

\section{kdump \& kexec}

As presented in the course, kdump/kexec allows to boot a new kernel and dump a
perfect copy of the crashed kernel (memory, registers, etc) which can be then
debugged using gdb or crash. 

\subsection{Building the dump-capture kernel}

We will now build the dump-capture kernel which will be booted on crash using
kexec. For that, we will use a simple buildroot image with a builtin initramfs
using the following commands on the development host:

\begin{bashinput}
$ cd /home/$USER/debugging-labs/buildroot
$ make O=build_kexec stm32mp157a_dk1_debugging_kexec_defconfig
$ make O=build_kexec
\end{bashinput}

Then, we'll copy the zImage and the device-tree to the nfs /root/kexec
directory:

\begin{bashinput}
$ mkdir /home/$USER/debugging-labs/nfsroot/root/kexec
$ cp build_kexec/images/zImage /home/$USER/debugging-labs/nfsroot/root/kexec
$ cp build_kexec/images/stm32mp157a-dk1.dtb /home/$USER/debugging-labs/nfsroot/root/kexec
\end{bashinput}

These files are now ready to be used from the target using kexec.

\subsection{Configuring kexec}

First of all we need to setup a kexec suitable memory zone for our crash kernel
image. This is achieved via the linux command line. Reboot, interrupt U-Boot and
add the \code{crashkernel=60M} parameter. This will tell the kernel to reserve
60M of memory to load a "rescue" kernel that will be booted on panic. We will
also add an option which will panic the kernel on oops to allow executing the
kexec kernel.

\begin{bashinput}
STM32MP> env edit bootargs
STM32MP> <existing bootargs> crashkernel=60M oops=panic
STM32MP> boot
\end{bashinput}

To load the crash kernel into the previously reserved memory zone, run the
following command on the target:

\begin{bashinput}
# kexec --type zImage -p /root/kexec/zImage --dtb=/root/kexec/stm32mp157a-dk1.dtb
  --command-line="console=ttySTM0,115200n8 maxcpus=1 reset_devices"
\end{bashinput}

Once done, you can trigger a crash using the previously mentioned watchdog
command:

\begin{bashinput}
$ watchdog -T 10 -t 5 /dev/watchdog0
\end{bashinput}

At this moment, the kernel will reboot into a new kernel using the specified
kernel after displaying the backtrace and a message:

\begin{bashinput}
[ 1181.987971] Loading crashdump kernel...
[ 1181.990839] Bye!
\end{bashinput}

After reboot, log into the new kernel normally and bring up the eth0 interface:
(We will use 192.168.0.101 to avoid cloberring ssh \code{know_hosts} file for
the 192.168.0.100 entry).
\begin{bashinput}
$ ifconfig eth0 192.168.0.101
\end{bashinput}

\textbf{Note:} ethernet setup might timeout due to some init issues after kexec
boot so this commands needs to be run another time.

\begin{bashinput}
$ cd /home/$USER/debugging-labs/
$ scp root@192.168.0.101:/proc/vmcore .
\end{bashinput}

You can load the \code{vmcore} file using cross GDB:

\begin{bashinput}
$ ${CROSS_COMPILE}gdb /home/$USER/debugging-labs/buildroot/output/build/linux-5.13/vmlinux \
  vmcore
\end{bashinput}
